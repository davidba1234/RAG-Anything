#!/usr/bin/env python
"""
RAG-Anything å¿«é€Ÿå…¥é—¨ç¤ºä¾‹

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ RAG-Anything è¿›è¡Œæ–‡æ¡£å¤„ç†å’ŒæŸ¥è¯¢ï¼š
1. åŸºæœ¬é…ç½®å’Œåˆå§‹åŒ–
2. å¤„ç†æ–‡æ¡£
3. æ‰§è¡Œæ–‡æœ¬æŸ¥è¯¢
4. æ‰§è¡Œå¤šæ¨¡æ€æŸ¥è¯¢

ä½¿ç”¨å‰è¯·ç¡®ä¿ï¼š
1. å·²é…ç½® .env æ–‡ä»¶ä¸­çš„ API å¯†é’¥
2. å‡†å¤‡äº†è¦å¤„ç†çš„æµ‹è¯•æ–‡æ¡£
"""

import os
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from raganything import RAGAnything, RAGAnythingConfig

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

async def main():
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤º RAG-Anything çš„åŸºæœ¬ä½¿ç”¨æµç¨‹
    """
    
    print("ğŸš€ RAG-Anything å¿«é€Ÿå…¥é—¨")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ç¯å¢ƒé…ç½®
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    llm_model = os.getenv("LLM_MODEL", "gpt-4o-mini")
    
    if not api_key:
        print("âŒ é…ç½®æ£€æŸ¥å¤±è´¥ï¼")
        print("\nğŸ“‹ é…ç½®å¸®åŠ©:")
        print("1. å¤åˆ¶ env.example ä¸º .env")
        print("2. æ¨èä½¿ç”¨ SiliconFlow + DeepSeek-V3.1 (æ€§ä»·æ¯”æœ€é«˜)")
        print("3. è¿è¡Œ 'python test_api_config.py' æµ‹è¯•é…ç½®")
        print("\nğŸ”— è·å–API Key:")
        print("â€¢ SiliconFlow: https://siliconflow.cn/")
        print("\nğŸŒŸ æ¨èé…ç½®:")
        print("OPENAI_API_KEY=sk-your-siliconflow-api-key")
        print("OPENAI_BASE_URL=https://api.siliconflow.cn/v1")
        print("OPENAI_MODEL=Pro/deepseek-ai/DeepSeek-V3.1")
        print("OPENAI_EMBEDDING_MODEL=Pro/BAAI/bge-m3")
        return
    
    # è¯†åˆ« API ä¾›åº”å•†
    supplier = "æœªé…ç½®"
    if base_url:
        if "siliconflow.cn" in base_url:
            supplier = "SiliconFlow"
        else:
            supplier = "å…¶ä»–ä¾›åº”å•†"
    
    print(f"âœ… API é…ç½®æ£€æŸ¥å®Œæˆ")
    print(f"   ä¾›åº”å•†: {supplier}")
    print(f"   Base URL: {base_url}")
    print(f"   Model: {llm_model}")
    print(f"   API Key: {api_key[:10]}...")
    
    # 2. åˆ›å»º RAGAnything é…ç½®
    config = RAGAnythingConfig(
        working_dir=os.getenv("WORKING_DIR", "./rag_storage"),
        parser=os.getenv("PARSER", "mineru"),
        parse_method=os.getenv("PARSE_METHOD", "auto"),
        enable_image_processing=os.getenv("ENABLE_IMAGE_PROCESSING", "true").lower() == "true",
        enable_table_processing=os.getenv("ENABLE_TABLE_PROCESSING", "true").lower() == "true",
        enable_equation_processing=os.getenv("ENABLE_EQUATION_PROCESSING", "true").lower() == "true",
    )
    
    print(f"âœ… RAG é…ç½®åˆ›å»ºå®Œæˆ")
    print(f"   å·¥ä½œç›®å½•: {config.working_dir}")
    print(f"   è§£æå™¨: {config.parser}")
    print(f"   è§£ææ–¹æ³•: {config.parse_method}")
    
    # 3. å®šä¹‰æ¨¡å‹å‡½æ•°
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        """LLM æ¨¡å‹å‡½æ•°"""
        return openai_complete_if_cache(
            os.getenv("LLM_MODEL", "gpt-4o-mini"),
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    
    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        """è§†è§‰æ¨¡å‹å‡½æ•°"""
        if messages:
            # å¤šæ¨¡æ€ VLM å¢å¼ºæŸ¥è¯¢æ ¼å¼
            return openai_complete_if_cache(
                os.getenv("VISION_MODEL", "gpt-4o"),
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        elif image_data:
            # ä¼ ç»Ÿå•å›¾åƒæ ¼å¼
            return openai_complete_if_cache(
                os.getenv("VISION_MODEL", "gpt-4o"),
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
                            },
                        ],
                    } if image_data else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        else:
            # çº¯æ–‡æœ¬æ ¼å¼
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)
    
    # 4. å®šä¹‰åµŒå…¥å‡½æ•°
    embedding_func = EmbeddingFunc(
        embedding_dim=int(os.getenv("EMBEDDING_DIM", "3072")),
        max_token_size=int(os.getenv("MAX_TOKEN_SIZE", "8192")),
        func=lambda texts: openai_embed(
            texts,
            model=os.getenv("EMBEDDING_MODEL", "text-embedding-3-large"),
            api_key=api_key,
            base_url=base_url,
        ),
    )
    
    print(f"âœ… æ¨¡å‹å‡½æ•°é…ç½®å®Œæˆ")
    
    # 5. åˆå§‹åŒ– RAGAnything
    try:
        rag = RAGAnything(
            config=config,
            llm_model_func=llm_model_func,
            vision_model_func=vision_model_func,
            embedding_func=embedding_func,
        )
        print(f"âœ… RAGAnything åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ RAGAnything åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # 6. æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•æ–‡æ¡£
    test_files = []
    for ext in ["*.pdf", "*.docx", "*.pptx", "*.txt", "*.md"]:
        test_files.extend(Path(".").glob(ext))
    
    if not test_files:
        print("\nğŸ“„ æœªæ‰¾åˆ°æµ‹è¯•æ–‡æ¡£")
        print("è¯·åœ¨å½“å‰ç›®å½•æ”¾ç½®ä¸€äº›æ–‡æ¡£æ–‡ä»¶ (PDF, DOCX, PPTX, TXT, MD)")
        print("\nğŸ’¡ ä½ å¯ä»¥ï¼š")
        print("   1. ä¸‹è½½ä¸€äº›ç¤ºä¾‹æ–‡æ¡£")
        print("   2. åˆ›å»ºä¸€ä¸ªç®€å•çš„æ–‡æœ¬æ–‡ä»¶è¿›è¡Œæµ‹è¯•")
        
        # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ–‡æ¡£
        sample_doc = Path("sample_document.txt")
        if not sample_doc.exists():
            sample_content = """
# RAG-Anything ç¤ºä¾‹æ–‡æ¡£

## ä»€ä¹ˆæ˜¯ RAG-Anythingï¼Ÿ

RAG-Anything æ˜¯ä¸€ä¸ªåŸºäº LightRAG çš„å¤šæ¨¡æ€æ–‡æ¡£å¤„ç†ç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿï¼š

1. **å¤šæ ¼å¼æ”¯æŒ**ï¼šå¤„ç† PDFã€Wordã€PowerPointã€å›¾ç‰‡ç­‰å¤šç§æ ¼å¼
2. **æ™ºèƒ½è§£æ**ï¼šä½¿ç”¨ MinerU å’Œ Docling è¿›è¡Œé«˜è´¨é‡æ–‡æ¡£è§£æ
3. **å¤šæ¨¡æ€æŸ¥è¯¢**ï¼šæ”¯æŒæ–‡æœ¬å’Œå›¾åƒçš„æ··åˆæŸ¥è¯¢
4. **å›¾è°±æ„å»º**ï¼šè‡ªåŠ¨æ„å»ºçŸ¥è¯†å›¾è°±ä»¥å¢å¼ºæ£€ç´¢æ•ˆæœ

## æ ¸å¿ƒç‰¹æ€§

- ç«¯åˆ°ç«¯æ–‡æ¡£å¤„ç†æµæ°´çº¿
- é«˜çº§å¤šæ¨¡æ€å†…å®¹ç†è§£
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ£€ç´¢ç³»ç»Ÿ
- æ‰¹å¤„ç†èƒ½åŠ›
- çµæ´»çš„é…ç½®é€‰é¡¹

## ä½¿ç”¨åœºæ™¯

1. å­¦æœ¯ç ”ç©¶æ–‡çŒ®åˆ†æ
2. ä¼ä¸šæ–‡æ¡£çŸ¥è¯†ç®¡ç†
3. æŠ€æœ¯æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
4. å¤šåª’ä½“å†…å®¹åˆ†æ

è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯• RAG-Anything åŠŸèƒ½çš„ç¤ºä¾‹æ–‡æ¡£ã€‚
            """
            sample_doc.write_text(sample_content, encoding='utf-8')
            print(f"\nâœ… å·²åˆ›å»ºç¤ºä¾‹æ–‡æ¡£: {sample_doc}")
            test_files = [sample_doc]
    
    if test_files:
        print(f"\nğŸ“„ æ‰¾åˆ° {len(test_files)} ä¸ªæµ‹è¯•æ–‡æ¡£:")
        for i, file in enumerate(test_files[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"   {i}. {file.name}")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡æ¡£è¿›è¡Œå¤„ç†
        test_file = test_files[0]
        print(f"\nğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£: {test_file.name}")
        
        try:
            # å¤„ç†æ–‡æ¡£
            await rag.process_document_complete(
                file_path=str(test_file),
                output_dir=os.getenv("OUTPUT_DIR", "./output"),
                parse_method=config.parse_method
            )
            print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ")
            
            # æ‰§è¡Œç¤ºä¾‹æŸ¥è¯¢
            print(f"\nğŸ” æ‰§è¡Œç¤ºä¾‹æŸ¥è¯¢...")
            
            # æ–‡æœ¬æŸ¥è¯¢ç¤ºä¾‹
            text_query = "è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
            print(f"\nğŸ“ æ–‡æœ¬æŸ¥è¯¢: {text_query}")
            
            text_result = await rag.aquery(
                query=text_query,
                mode="hybrid"
            )
            print(f"\nğŸ’¬ æŸ¥è¯¢ç»“æœ:")
            print(f"{text_result}")
            
            print(f"\nğŸ‰ å¿«é€Ÿå…¥é—¨æ¼”ç¤ºå®Œæˆï¼")
            print(f"\nğŸ“š æ¥ä¸‹æ¥ä½ å¯ä»¥ï¼š")
            print(f"   1. å°è¯•ä¸åŒçš„æŸ¥è¯¢é—®é¢˜")
            print(f"   2. å¤„ç†æ›´å¤šæ–‡æ¡£")
            print(f"   3. æ¢ç´¢å¤šæ¨¡æ€æŸ¥è¯¢åŠŸèƒ½")
            print(f"   4. æŸ¥çœ‹ examples/ ç›®å½•ä¸­çš„æ›´å¤šç¤ºä¾‹")
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            print(f"\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š")
            print(f"   1. æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®")
            print(f"   2. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸")
            print(f"   3. æ£€æŸ¥æ–‡æ¡£æ ¼å¼æ˜¯å¦æ”¯æŒ")

if __name__ == "__main__":
    print("å¯åŠ¨ RAG-Anything å¿«é€Ÿå…¥é—¨...")
    asyncio.run(main())